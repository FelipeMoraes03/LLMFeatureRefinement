{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All root features\n",
    "# with open(\"../data/root_features.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     root_features = json.load(file)\n",
    "\n",
    "# Selected root features\n",
    "with open(\"../data/selected_features.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    root_features = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\"\n",
    "\"You are an expert in mobile app development and requirements engineering. \n",
    "You excel at decomposing high-level features into detailed sub-features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level1_prompt(feature: str, feature_description: str, num_features: int = 5, technique: str = \"base\", examples: List[dict] = None) -> str:\n",
    "\n",
    "    base_prompt = f\"\"\"\n",
    "        **Feature**\n",
    "        ```\n",
    "        {feature}: {feature_description}\n",
    "        ```\n",
    "        Given the mobile app feature above, please refine it to a list of sub-features.\n",
    "        Ensure that the number of sub-features is {num_features}.\n",
    "        The output should be a list of JSON formatted objects like this:\n",
    "        [{{\n",
    "        \"sub_feature\": sub_feature,\n",
    "        \"description\": description\n",
    "        }}]\n",
    "        \"\"\"\n",
    "    \n",
    "    if technique == \"base\":\n",
    "        return base_prompt\n",
    "\n",
    "    elif technique == \"chain_of_thought\":\n",
    "        return base_prompt + \"\\nLet's break this down step by step to identify sub-features. Think carefully before listing them.\"\n",
    "\n",
    "    elif technique == \"few_shot\":\n",
    "        examples_str = \"\\n\".join([f\"Example:\\nFeature: {ex['feature']}\\nSub-features: {ex['sub_features']}\" for ex in examples])\n",
    "        return f\"\"\"\n",
    "    {examples_str}\n",
    "\n",
    "    {base_prompt}\n",
    "    \"\"\"\n",
    "\n",
    "    elif technique == \"contrastive_thinking\":\n",
    "        return base_prompt + \"\\nPropose two contrasting interpretations of the feature, then derive sub-features for each interpretation.\"\n",
    "\n",
    "    elif technique == \"prompt_maieutica\":\n",
    "        return base_prompt + \"\\nLet's use a Socratic approach: What is the primary goal of this feature? What are the essential components? How do these components interact?\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompt technique\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level2_prompt(feature: str, feature_description: str, super_feature: str, super_feature_description: str,\n",
    "                      siblings_features: List[str], num_features: int = 5, technique: str = \"base\", examples: List[dict] = None) -> str:\n",
    "    base_prompt = f\"\"\"\n",
    "**Super Feature**\n",
    "```\n",
    "super-feature: {super_feature}\n",
    "description: {super_feature_description}\n",
    "```\n",
    "Knowing that the feature \"{super_feature}\" above is refined into a list of the following features:\n",
    "```\n",
    "{siblings_features}\n",
    "```\n",
    "\n",
    "Please refine the following to a list of sub-features.\n",
    "Ensure that the number of sub-features is {num_features}.\n",
    "\n",
    "**Feature**\n",
    "```\n",
    "feature: {feature}\n",
    "description: {feature_description}\n",
    "```\n",
    "\n",
    "The output should be a list of JSON formatted objects like this:\n",
    "[\n",
    "    {{\n",
    "        \"sub_feature\": sub_feature,\n",
    "        \"description\": description\n",
    "    }}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "    if technique == \"base\":\n",
    "        return base_prompt\n",
    "\n",
    "    elif technique == \"chain_of_thought\":\n",
    "        return base_prompt + \"\\nLet's break this down step by step to identify sub-features. Think carefully before listing them.\"\n",
    "\n",
    "    elif technique == \"few_shot\":\n",
    "        examples_str = \"\\n\".join([f\"Example:\\nSuper Feature: {ex['super_feature']}\\nSuper Feature Description: {ex['super_feature_description']}\\nFeature: {ex['feature']}\\nFeature Description: {ex['feature_description']}\\nSub-features: {ex['sub_features']}\" for ex in examples])\n",
    "        return f\"\"\"\n",
    "    {examples_str}\n",
    "\n",
    "    {base_prompt}\n",
    "    \"\"\"\n",
    "\n",
    "    elif technique == \"contrastive_thinking\":\n",
    "        return base_prompt + \"\\nPropose two contrasting interpretations of the feature, then derive sub-features for each interpretation.\"\n",
    "\n",
    "    elif technique == \"prompt_maieutica\":\n",
    "        return base_prompt + \"\\nLet's use a Socratic approach: What is the primary goal of this feature? What are the essential components? How do these components interact?\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompt technique\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples_from_json(directory: str, filename: str, level: int) -> List[dict]:\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            examples = json.load(file)\n",
    "\n",
    "        if level == 1:\n",
    "            formatted_examples = [\n",
    "                {\n",
    "                    \"feature\": example[\"feature\"],\n",
    "                    \"sub_features\": json.dumps(example[\"sub_features\"], indent=2)\n",
    "                }\n",
    "                for example in examples\n",
    "            ]\n",
    "        \n",
    "        elif level == 2:\n",
    "            formatted_examples = [\n",
    "                {\n",
    "                    \"super_feature\": example[\"super_feature\"],\n",
    "                    \"super_feature_description\": example[\"super_feature_description\"],\n",
    "                    \"siblings_features\": example[\"siblings_features\"],\n",
    "                    \"feature\": example[\"feature\"],\n",
    "                    \"feature_description\": example[\"feature_description\"],\n",
    "                    \"sub_features\": json.dumps(example[\"sub_features\"], indent=2)\n",
    "                }\n",
    "                for example in examples\n",
    "            ]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"O parâmetro 'level' deve ser 1 ou 2.\")\n",
    "\n",
    "        return formatted_examples\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo '{filename}' não encontrado no diretório '{directory}'.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Erro: Não foi possível decodificar o JSON no arquivo '{filename}'.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_techniques = [\"base\", \"chain_of_thought\", \"few_shot\", \"contrastive_thinking\", \"prompt_maieutica\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1 Feature Refinements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level_results = []\n",
    "\n",
    "for feature in root_features:\n",
    "    prompt = get_level1_prompt(\n",
    "        feature[\"feature\"],\n",
    "        feature[\"description\"]\n",
    "    )\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        config=types.GenerateContentConfig(system_instruction=system_prompt),\n",
    "        contents=prompt\n",
    "    )\n",
    "\n",
    "    first_level_results.append({\n",
    "        \"feature\": feature[\"feature\"],\n",
    "        \"description\": feature[\"description\"],\n",
    "        \"sub_features\": json.loads((response.text).replace('```', '').replace('json', ''))\n",
    "    })\n",
    "\n",
    "    # Minimizar problemas de limite de requisicao por minuto\n",
    "    time.sleep(5)\n",
    "\n",
    "# Salvar os resultados em um arquivo JSON\n",
    "with open(\"../data/gemini/base_prompt/first_level_subfeatures.json\", \"w\") as f:\n",
    "    json.dump(first_level_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2 Feature Refinements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_results = []\n",
    "\n",
    "for super_feature in first_level_results:\n",
    "    for feature in super_feature[\"sub_features\"]:\n",
    "        prompt = get_level2_prompt(\n",
    "            feature[\"sub_feature\"], \n",
    "            feature[\"description\"],\n",
    "            super_feature[\"feature\"],\n",
    "            super_feature[\"description\"],\n",
    "            [f[\"sub_feature\"] for f in super_feature[\"sub_features\"]]\n",
    "        )\n",
    "\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            config=types.GenerateContentConfig(system_instruction=system_prompt),\n",
    "            contents=prompt\n",
    "        )\n",
    "\n",
    "        second_level_results.append({\n",
    "            \"super_feature\": super_feature[\"feature\"],\n",
    "            \"super_feature_description\": super_feature[\"description\"],\n",
    "            \"feature\": feature[\"sub_feature\"],\n",
    "            \"description\": feature[\"description\"],\n",
    "            \"siblings\": [f[\"sub_feature\"] for f in super_feature[\"sub_features\"]],\n",
    "            \"sub_features\": json.loads((response.text).replace('```', '').replace('json', ''))\n",
    "        })\n",
    "\n",
    "        # Minimizar problemas de limite de requisicao por minuto\n",
    "        time.sleep(5)\n",
    "\n",
    "# Salvar os resultados em um arquivo JSON\n",
    "with open(\"../data/gemini/base_prompt/second_level_subfeatures.json\", \"w\") as f:\n",
    "    json.dump(second_level_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
