{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All root features\n",
    "# with open(\"../data/root_features.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     root_features = json.load(file)\n",
    "\n",
    "# Selected root features\n",
    "with open(\"../data/selected_features.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    root_features = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\"\n",
    "\"You are an expert in mobile app development and requirements engineering. \n",
    "You excel at decomposing high-level features into detailed sub-features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_format = \"\"\"\n",
    "\n",
    "    Do not replicate the results of the article \"Getting Inspiration for Feature Elicitation: App Store- vs. LLM-based Approach\", think for yourself to do the feature refinement.\n",
    "    \n",
    "    The output should be a list of JSON formatted objects like this:\n",
    "    [\n",
    "        {{\n",
    "            \"sub_feature\": sub_feature,\n",
    "            \"description\": description\n",
    "        }}\n",
    "    ]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level1_prompt(feature: str, feature_description: str, num_features: int = 5, technique: str = \"base\", examples: List[dict] = None) -> str:\n",
    "\n",
    "    base_prompt = f\"\"\"\n",
    "        **Feature**\n",
    "        ```\n",
    "        {feature}: {feature_description}\n",
    "        ```\n",
    "        Given the mobile app feature above, please refine it to a list of sub-features.\n",
    "        Ensure that the number of sub-features is {num_features}.\n",
    "        \"\"\"\n",
    "    \n",
    "    if technique == \"base\":\n",
    "        return base_prompt + answer_format\n",
    "\n",
    "    elif technique == \"chain_of_thought\":\n",
    "        return base_prompt + \"\\nLet's break this down step by step to identify sub-features. Think carefully before listing them. Only return the JSON as answer, as specified.\" + answer_format\n",
    "\n",
    "    elif technique == \"few_shot\":\n",
    "        examples_str = \"\\n\".join([f\"Example:\\nFeature: {ex['feature']}\\nSub-features: {ex['sub_features']}\" for ex in examples])\n",
    "        return f\"\"\"\n",
    "    {examples_str}\n",
    "\n",
    "    {base_prompt}\n",
    "    {answer_format}\n",
    "    \"\"\"\n",
    "\n",
    "    elif technique == \"prompt_maieutica\":\n",
    "        return base_prompt + \"\\nLet's use a Socratic approach: What is the primary goal of this feature? What are the essential components? How do these components interact? Only return the JSON as answer, as specified.\" + answer_format\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompt technique\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level2_prompt(feature: str, feature_description: str, super_feature: str, super_feature_description: str,\n",
    "                      siblings_features: List[str], num_features: int = 5, technique: str = \"base\", examples: List[dict] = None) -> str:\n",
    "    base_prompt = f\"\"\"\n",
    "**Super Feature**\n",
    "```\n",
    "super-feature: {super_feature}\n",
    "description: {super_feature_description}\n",
    "```\n",
    "Knowing that the feature \"{super_feature}\" above is refined into a list of the following features:\n",
    "```\n",
    "{siblings_features}\n",
    "```\n",
    "\n",
    "Please refine the following to a list of sub-features.\n",
    "Ensure that the number of sub-features is {num_features}.\n",
    "\n",
    "**Feature**\n",
    "```\n",
    "feature: {feature}\n",
    "description: {feature_description}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    if technique == \"base\":\n",
    "        return base_prompt + answer_format\n",
    "\n",
    "    elif technique == \"chain_of_thought\":\n",
    "        return base_prompt + \"\\nLet's break this down step by step to identify sub-features. Think carefully before listing them. Only return the JSON as answer, as specified.\" +  answer_format\n",
    "\n",
    "    elif technique == \"few_shot\":\n",
    "        examples_str = \"\\n\".join([f\"Example:\\nSuper Feature: {ex['super_feature']}\\nSuper Feature Description: {ex['super_feature_description']}\\nFeature: {ex['feature']}\\nFeature Description: {ex['feature_description']}\\nSub-features: {ex['sub_features']}\" for ex in examples])\n",
    "        return f\"\"\"\n",
    "    {examples_str}\n",
    "\n",
    "    {base_prompt}\n",
    "    {answer_format}\n",
    "    \"\"\"\n",
    "    \n",
    "    elif technique == \"prompt_maieutica\":\n",
    "        return base_prompt + \"\\nLet's use a Socratic approach: What is the primary goal of this feature? What are the essential components? How do these components interact? Only return the JSON as answer, as specified.\" + answer_format\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompt technique\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples_from_json(directory: str, filename: str, level: int) -> List[dict]:\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            examples = json.load(file)\n",
    "\n",
    "        if level == 1:\n",
    "            formatted_examples = [\n",
    "                {\n",
    "                    \"feature\": example[\"feature\"],\n",
    "                    \"sub_features\": json.dumps(example[\"sub_features\"], indent=2)\n",
    "                }\n",
    "                for example in examples\n",
    "            ]\n",
    "        \n",
    "        elif level == 2:\n",
    "            formatted_examples = [\n",
    "                {\n",
    "                    \"super_feature\": example[\"super_feature\"],\n",
    "                    \"super_feature_description\": example[\"super_feature_description\"],\n",
    "                    \"siblings_features\": example[\"siblings_features\"],\n",
    "                    \"feature\": example[\"feature\"],\n",
    "                    \"feature_description\": example[\"feature_description\"],\n",
    "                    \"sub_features\": json.dumps(example[\"sub_features\"], indent=2)\n",
    "                }\n",
    "                for example in examples\n",
    "            ]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"O parâmetro 'level' deve ser 1 ou 2.\")\n",
    "\n",
    "        return formatted_examples\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo '{filename}' não encontrado no diretório '{directory}'.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Erro: Não foi possível decodificar o JSON no arquivo '{filename}'.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_techniques = [\"base\", \"chain_of_thought\", \"few_shot\", \"prompt_maieutica\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1 Feature Refinements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "techniques_first_level_results = {}\n",
    "for technique in prompt_techniques:\n",
    "        \n",
    "    first_level_results = []\n",
    "\n",
    "    for feature in root_features:\n",
    "        prompt = get_level1_prompt(\n",
    "            feature[\"feature\"],\n",
    "            feature[\"description\"],\n",
    "            technique=technique,\n",
    "            examples=load_examples_from_json(\"../data/examples\", \"few_shot_examples_level_1.json\", 1) if technique == \"few_shot\" else None\n",
    "        )\n",
    "\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            config=types.GenerateContentConfig(system_instruction=system_prompt),\n",
    "            contents=prompt\n",
    "        )\n",
    "        \n",
    "        first_level_results.append({\n",
    "            \"feature\": feature[\"feature\"],\n",
    "            \"description\": feature[\"description\"],\n",
    "            \"sub_features\": json.loads((response.text).replace('```', '').replace('json', ''))\n",
    "        })\n",
    "\n",
    "        # Minimizar problemas de limite de requisicao por minuto\n",
    "        time.sleep(5)\n",
    "\n",
    "    techniques_first_level_results[technique] = first_level_results\n",
    "\n",
    "    # Salvar os resultados em um arquivo JSON\n",
    "    with open(f\"../data/gemini/{technique}/first_level_subfeatures.json\", \"w\") as f:\n",
    "        json.dump(first_level_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2 Feature Refinements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ServerError",
     "evalue": "503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mServerError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 17\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m super_feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_features\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      7\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m get_level2_prompt(\n\u001b[0;32m      8\u001b[0m         feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_feature\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m      9\u001b[0m         feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m         examples\u001b[38;5;241m=\u001b[39mload_examples_from_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/examples\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfew_shot_examples_level_2.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m technique \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfew_shot\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     )\n\u001b[1;32m---> 17\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-2.0-flash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerateContentConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem_instruction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     second_level_results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuper_feature\u001b[39m\u001b[38;5;124m\"\u001b[39m: super_feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuper_feature_description\u001b[39m\u001b[38;5;124m\"\u001b[39m: super_feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mloads((response\u001b[38;5;241m.\u001b[39mtext)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     30\u001b[0m     })\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Minimizar problemas de limite de requisicao por minuto\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\biaar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\genai\\models.py:5370\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[1;34m(self, model, contents, config)\u001b[0m\n\u001b[0;32m   5368\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   5369\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 5370\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5371\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[0;32m   5372\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5373\u001b[0m   logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   5374\u001b[0m   remaining_remote_calls_afc \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\biaar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\genai\\models.py:4338\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[1;34m(self, model, contents, config)\u001b[0m\n\u001b[0;32m   4335\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[0;32m   4336\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[1;32m-> 4338\u001b[0m response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[0;32m   4340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mvertexai:\n\u001b[0;32m   4343\u001b[0m   response_dict \u001b[38;5;241m=\u001b[39m _GenerateContentResponse_from_vertex(\n\u001b[0;32m   4344\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client, response_dict\n\u001b[0;32m   4345\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\biaar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\genai\\_api_client.py:640\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[1;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    632\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    635\u001b[0m     http_options: Optional[HttpOptionsOrDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    636\u001b[0m ):\n\u001b[0;32m    637\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[0;32m    638\u001b[0m       http_method, path, request_dict, http_options\n\u001b[0;32m    639\u001b[0m   )\n\u001b[1;32m--> 640\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m   json_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson\n\u001b[0;32m    642\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "File \u001b[1;32mc:\\Users\\biaar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\genai\\_api_client.py:569\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[1;34m(self, http_request, stream)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    562\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpx_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    563\u001b[0m       method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    564\u001b[0m       url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    567\u001b[0m       timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    568\u001b[0m   )\n\u001b[1;32m--> 569\u001b[0m   \u001b[43merrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    570\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[0;32m    571\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[0;32m    572\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\biaar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\genai\\errors.py:108\u001b[0m, in \u001b[0;36mAPIError.raise_for_response\u001b[1;34m(cls, response)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[1;32m--> 108\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(status_code, response)\n",
      "\u001b[1;31mServerError\u001b[0m: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}"
     ]
    }
   ],
   "source": [
    "for technique in prompt_techniques:\n",
    "    second_level_results = []\n",
    "    first_level_results = techniques_first_level_results[technique]\n",
    "\n",
    "    for super_feature in first_level_results:\n",
    "        for feature in super_feature[\"sub_features\"]:\n",
    "            prompt = get_level2_prompt(\n",
    "                feature[\"sub_feature\"], \n",
    "                feature[\"description\"],\n",
    "                super_feature[\"feature\"],\n",
    "                super_feature[\"description\"],\n",
    "                siblings_features=[f[\"sub_feature\"] for f in super_feature[\"sub_features\"]],\n",
    "                technique=technique,\n",
    "                examples=load_examples_from_json(\"../data/examples\", \"few_shot_examples_level_2.json\", 2) if technique == \"few_shot\" else None\n",
    "            )\n",
    "\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                config=types.GenerateContentConfig(system_instruction=system_prompt),\n",
    "                contents=prompt\n",
    "            )\n",
    "\n",
    "            second_level_results.append({\n",
    "                \"super_feature\": super_feature[\"feature\"],\n",
    "                \"super_feature_description\": super_feature[\"description\"],\n",
    "                \"feature\": feature[\"sub_feature\"],\n",
    "                \"description\": feature[\"description\"],\n",
    "                \"siblings\": [f[\"sub_feature\"] for f in super_feature[\"sub_features\"]],\n",
    "                \"sub_features\": json.loads((response.text).replace('```', '').replace('json', ''))\n",
    "            })\n",
    "\n",
    "            # Minimizar problemas de limite de requisicao por minuto\n",
    "            time.sleep(10)\n",
    "\n",
    "    # Salvar os resultados em um arquivo JSON\n",
    "    with open(f\"../data/gemini/{technique}/second_level_subfeatures.json\", \"w\") as f:\n",
    "        json.dump(second_level_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
