{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = \"selected_features\"\n",
    "\n",
    "prompt_technique = \"base\"\n",
    "prompt_examples_path_l1 = \"\"\n",
    "prompt_examples_path_l2 = \"\"\n",
    "\n",
    "# llm = \"gpt\"\n",
    "llm = \"gemini\"\n",
    "\n",
    "# model = \"gpt-4o-mini\"\n",
    "model = \"gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_gemini = genai.Client(api_key=gemini_api_key)\n",
    "client_openai = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All root features\n",
    "# with open(\"../data/root_features.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     root_features = json.load(file)\n",
    "\n",
    "# Selected root features\n",
    "with open(\"../data/selected_features.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    root_features = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\"\n",
    "\"You are an expert in mobile app development and requirements engineering. \n",
    "You excel at decomposing high-level features into detailed sub-features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_format = \"\"\"\n",
    "\n",
    "    Do not replicate the results of the article \"Getting Inspiration for Feature Elicitation: App Store- vs. LLM-based Approach\", think for yourself to do the feature refinement.\n",
    "    \n",
    "    The output should be a list of JSON formatted objects like this:\n",
    "    [\n",
    "        {{\n",
    "            \"sub_feature\": sub_feature,\n",
    "            \"description\": description\n",
    "        }}\n",
    "    ]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level1_prompt(feature: str, feature_description: str, num_features: int = 5, technique: str = \"base\", examples: List[dict] = None) -> str:\n",
    "\n",
    "    base_prompt = f\"\"\"\n",
    "        **Feature**\n",
    "        ```\n",
    "        {feature}: {feature_description}\n",
    "        ```\n",
    "        Given the mobile app feature above, please refine it to a list of sub-features.\n",
    "        Ensure that the number of sub-features is {num_features}.\n",
    "        \"\"\"\n",
    "    \n",
    "    if technique == \"base\":\n",
    "        return base_prompt + answer_format\n",
    "\n",
    "    elif technique == \"chain_of_thought\":\n",
    "        return base_prompt + \"\\nLet's break this down step by step to identify sub-features. Think carefully before listing them. Only return the JSON as answer, as specified.\" + answer_format\n",
    "\n",
    "    elif technique == \"few_shot\":\n",
    "        examples_str = \"\\n\".join([f\"Example:\\nFeature: {ex['feature']}\\nSub-features: {ex['sub_features']}\" for ex in examples])\n",
    "        return f\"\"\"\n",
    "    {examples_str}\n",
    "\n",
    "    {base_prompt}\n",
    "    {answer_format}\n",
    "    \"\"\"\n",
    "\n",
    "    elif technique == \"prompt_maieutica\":\n",
    "        return base_prompt + \"\\nLet's use a Socratic approach: What is the primary goal of this feature? What are the essential components? How do these components interact? Only return the JSON as answer, as specified.\" + answer_format\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompt technique\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level2_prompt(feature: str, feature_description: str, super_feature: str, super_feature_description: str,\n",
    "                      siblings_features: List[str], num_features: int = 5, technique: str = \"base\", examples: List[dict] = None) -> str:\n",
    "    base_prompt = f\"\"\"\n",
    "**Super Feature**\n",
    "```\n",
    "super-feature: {super_feature}\n",
    "description: {super_feature_description}\n",
    "```\n",
    "Knowing that the feature \"{super_feature}\" above is refined into a list of the following features:\n",
    "```\n",
    "{siblings_features}\n",
    "```\n",
    "\n",
    "Please refine the following to a list of sub-features.\n",
    "Ensure that the number of sub-features is {num_features}.\n",
    "\n",
    "**Feature**\n",
    "```\n",
    "feature: {feature}\n",
    "description: {feature_description}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    if technique == \"base\":\n",
    "        return base_prompt + answer_format\n",
    "\n",
    "    elif technique == \"chain_of_thought\":\n",
    "        return base_prompt + \"\\nLet's break this down step by step to identify sub-features. Think carefully before listing them. Only return the JSON as answer, as specified.\" +  answer_format\n",
    "\n",
    "    elif technique == \"few_shot\":\n",
    "        examples_str = \"\\n\".join([f\"Example:\\nSuper Feature: {ex['super_feature']}\\nSuper Feature Description: {ex['super_feature_description']}\\nFeature: {ex['feature']}\\nFeature Description: {ex['feature_description']}\\nSub-features: {ex['sub_features']}\" for ex in examples])\n",
    "        return f\"\"\"\n",
    "    {examples_str}\n",
    "\n",
    "    {base_prompt}\n",
    "    {answer_format}\n",
    "    \"\"\"\n",
    "    \n",
    "    elif technique == \"prompt_maieutica\":\n",
    "        return base_prompt + \"\\nLet's use a Socratic approach: What is the primary goal of this feature? What are the essential components? How do these components interact? Only return the JSON as answer, as specified.\" + answer_format\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompt technique\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples_from_json(directory: str, filename: str, level: int) -> List[dict]:\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            examples = json.load(file)\n",
    "\n",
    "        if level == 1:\n",
    "            formatted_examples = [\n",
    "                {\n",
    "                    \"feature\": example[\"feature\"],\n",
    "                    \"sub_features\": json.dumps(example[\"sub_features\"], indent=2)\n",
    "                }\n",
    "                for example in examples\n",
    "            ]\n",
    "        \n",
    "        elif level == 2:\n",
    "            formatted_examples = [\n",
    "                {\n",
    "                    \"super_feature\": example[\"super_feature\"],\n",
    "                    \"super_feature_description\": example[\"super_feature_description\"],\n",
    "                    \"siblings_features\": example[\"siblings_features\"],\n",
    "                    \"feature\": example[\"feature\"],\n",
    "                    \"feature_description\": example[\"feature_description\"],\n",
    "                    \"sub_features\": json.dumps(example[\"sub_features\"], indent=2)\n",
    "                }\n",
    "                for example in examples\n",
    "            ]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"O parâmetro 'level' deve ser 1 ou 2.\")\n",
    "\n",
    "        return formatted_examples\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo '{filename}' não encontrado no diretório '{directory}'.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Erro: Não foi possível decodificar o JSON no arquivo '{filename}'.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prompt_examples_path_l1 == \"\":\n",
    "    prompt_examples_l1 = None\n",
    "else:\n",
    "    prompt_examples_l1 = load_examples_from_json(\"../data/examples\", f\"{prompt_examples_path_l1}.json\", 1)\n",
    "\n",
    "if prompt_examples_path_l2 == \"\":\n",
    "    prompt_examples_l2 = None\n",
    "else:\n",
    "    prompt_examples_l2 = load_examples_from_json(\"../data/examples\", f\"{prompt_examples_path_l2}.json\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1 Feature Refinements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level_results = []\n",
    "\n",
    "for feature in root_features:\n",
    "    prompt = get_level1_prompt(\n",
    "        feature[\"feature\"],\n",
    "        feature[\"description\"],\n",
    "        technique=prompt_technique,\n",
    "        examples=prompt_examples_l1\n",
    "    )\n",
    "\n",
    "    if llm == \"gemini\":\n",
    "        response = client_gemini.models.generate_content(\n",
    "            model=model,\n",
    "            config=types.GenerateContentConfig(system_instruction=system_prompt),\n",
    "            contents=prompt\n",
    "        )\n",
    "        message = response.text\n",
    "        \n",
    "        time.sleep(5) # Minimize issues with request limit per minute\n",
    "    elif llm == \"gpt\":\n",
    "        response = client_openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "        )\n",
    "        message = response.choices[0].message.content\n",
    "    else:\n",
    "        raise ValueError(\"Error: Invalid Model.\")\n",
    "    \n",
    "    first_level_results.append({\n",
    "        \"feature\": feature[\"feature\"],\n",
    "        \"description\": feature[\"description\"],\n",
    "        \"sub_features\": json.loads((message).replace('```', '').replace('json', ''))\n",
    "    })\n",
    "\n",
    "# Saving results into a JSON file\n",
    "with open(f\"../data/{model}/{prompt_technique}/first_level_subfeatures.json\", \"w\") as f:\n",
    "    json.dump(first_level_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2 Feature Refinements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Level Features already Loaded.\n"
     ]
    }
   ],
   "source": [
    "first_level_path = f\"../data/{model}/{prompt_technique}/first_level_subfeatures.json\"\n",
    "try:\n",
    "    if first_level_results:\n",
    "        print(\"First Level Features already Loaded.\")\n",
    "except:\n",
    "    if os.path.exists(first_level_path):\n",
    "        with open(first_level_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            first_level_results = json.load(file)\n",
    "        print(\"Loading First Level Features.\")\n",
    "    else:\n",
    "        print(\"First Level Features does not exists, run the notebook from start.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_results = []\n",
    "\n",
    "for super_feature in first_level_results:\n",
    "    for feature in super_feature[\"sub_features\"]:\n",
    "        prompt = get_level2_prompt(\n",
    "            feature[\"sub_feature\"], \n",
    "            feature[\"description\"],\n",
    "            super_feature[\"feature\"],\n",
    "            super_feature[\"description\"],\n",
    "            siblings_features=[f[\"sub_feature\"] for f in super_feature[\"sub_features\"]],\n",
    "            technique=prompt_technique,\n",
    "            examples=prompt_examples_l2\n",
    "        )\n",
    "\n",
    "        if llm == \"gemini\":\n",
    "            response = client_gemini.models.generate_content(\n",
    "                model=model,\n",
    "                config=types.GenerateContentConfig(system_instruction=system_prompt),\n",
    "                contents=prompt\n",
    "            )\n",
    "            message = response.text\n",
    "\n",
    "            time.sleep(5) # Minimize issues with request limit per minute\n",
    "        elif llm == \"gpt\":\n",
    "            response = client_openai.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "            )\n",
    "            message = response.choices[0].message.content\n",
    "        else:\n",
    "            raise ValueError(\"Error: Invalid Model.\")\n",
    "\n",
    "        second_level_results.append({\n",
    "            \"super_feature\": super_feature[\"feature\"],\n",
    "            \"super_feature_description\": super_feature[\"description\"],\n",
    "            \"feature\": feature[\"sub_feature\"],\n",
    "            \"description\": feature[\"description\"],\n",
    "            \"siblings\": [f[\"sub_feature\"] for f in super_feature[\"sub_features\"]],\n",
    "            \"sub_features\": json.loads((message).replace('```', '').replace('json', ''))\n",
    "        })\n",
    "\n",
    "# Saving results into a JSON file\n",
    "with open(f\"../data/{model}/{prompt_technique}/second_level_subfeatures.json\", \"w\") as f:\n",
    "    json.dump(second_level_results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
