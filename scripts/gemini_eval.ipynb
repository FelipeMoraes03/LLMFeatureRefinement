{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gemini'\n",
    "prompt_technique = 'base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/{model}/{prompt_technique}_prompt/first_level_subfeatures.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    first_level_subfeatures = json.load(file)\n",
    "\n",
    "with open(f\"../data/{model}/{prompt_technique}_prompt/second_level_subfeatures.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    second_level_subfeatures = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalution Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/eval_criteria.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    eval_criteria = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert in mobile app development and requirements engineering.  \n",
    "Your primary responsibility is to **critically evaluate** the refinement of software features, ensuring that the breakdown of high-level features into sub-features is **logical, precise, relevant, and technically feasible**.  \n",
    "\n",
    "Your evaluation must adhere **strictly** to the following criteria, applying **a rigorous standard** in your assessment:  \n",
    "\n",
    "### **Evaluation Criteria**\n",
    "\n",
    "1. **Feature Relationship Classification**:\n",
    "   - Classify each sub-feature in relation to the root feature using one of the following:\n",
    "     - **Sub-feature**: A feature that is directly derived from and dependent on the root feature.\n",
    "     - **Sibling feature**: A feature at the same hierarchical level as the root feature.\n",
    "     - **Super feature**: A feature that is broader or encompasses the root feature.\n",
    "     - **Identical feature**: A feature that is essentially the same as the root feature.\n",
    "     - **Other**: A feature that does not fit into the above categories.\n",
    "\n",
    "**Be precise in classification. Misclassification should be flagged.** \n",
    "\n",
    "-----\n",
    "\n",
    "2. **Relevance (Rating: 1 to 5)**:\n",
    "   - Evaluate how relevant the sub-feature is to the root feature:\n",
    "     - 5: Highly relevant and a natural extension.\n",
    "     - 4: Mostly relevant and logically connected.\n",
    "     - 3: Moderately relevant but might not serve the same purpose.\n",
    "     - 2: Somewhat relevant, mainly because it belongs to the same app category.\n",
    "     - 1: Not relevant at all.\n",
    "\n",
    "**A score of 4 or 5 requires a clear justification. A score of 1 or 2 must explain why the sub-feature does not align.**  \n",
    "\n",
    "-----\n",
    "\n",
    "3. **Clarity (Rating: 1 to 5)**:\n",
    "   - Assess the clarity and understandability of the sub-feature description:\n",
    "     - 5: Very clear and easily understandable.\n",
    "     - 4: Mostly clear with minor syntax issues.\n",
    "     - 3: Somewhat clear but contains ambiguities or is too lengthy.\n",
    "     - 2: Mostly unclear and difficult to understand.\n",
    "     - 1: Very unclear or irrelevant.\n",
    "\n",
    "**If the description lacks sufficient detail or is ambiguous, clarity should not exceed 3.**\n",
    "\n",
    "-----\n",
    "\n",
    "4. **Feasibility (Rating: 1 to 5)**:\n",
    "   - Evaluate how practical and implementable the sub-feature is:\n",
    "     - 5: Feasible and commonly implemented in existing apps.\n",
    "     - 4: Feasible but lacks clear real-world examples.\n",
    "     - 3: Probably feasible but has some uncertainties.\n",
    "     - 2: Probably not feasible due to technical limitations.\n",
    "     - 1: Not feasible at all.\n",
    "\n",
    "**If feasibility is uncertain, assume a **cautious** stance. Avoid overestimating feasibility.**\n",
    "\n",
    "-----\n",
    "\n",
    "**Instructions**:\n",
    "- Provide a structured evaluation for each sub-feature.\n",
    "- **Use the exact format** provided below for your response. \n",
    "- **Do not provide justifications** or explanations for the ratings or classifications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(feature: str, feature_description: str, super_feature: str, super_feature_description: str,) -> str:\n",
    "    return f\"\"\"\n",
    "Given the following mobile app feature and its refined sub-feature, evaluate the sub-feature based on the provided system guidelines.\n",
    "\n",
    "**Feature**\n",
    "```\n",
    "feature: {super_feature}\n",
    "description: {super_feature_description}\n",
    "```\n",
    "\n",
    "**Sub-feature to Evaluate**\n",
    "```\n",
    "sub_feature: {feature}\n",
    "description: {feature_description}\n",
    "```\n",
    "\n",
    "Provide a structured evaluation of this sub-feature, based on provided criteria.\n",
    "\n",
    "Return the evaluation as a structured JSON object in the following format:\n",
    "{{\n",
    "    \"relationship\": \"Sub-feature | Sibling feature | Super feature | Identical feature | Other\",\n",
    "    \"relevance\": Rating (1-5),\n",
    "    \"clarity\": Rating (1-5),\n",
    "    \"feasibility\": Rating (1-5)\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1 Feature Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level_eval = []\n",
    "\n",
    "for super_feature in first_level_subfeatures:\n",
    "    for feature in super_feature[\"sub_features\"]:\n",
    "        prompt = get_prompt(\n",
    "            feature[\"sub_feature\"], \n",
    "            feature[\"description\"],\n",
    "            super_feature[\"feature\"],\n",
    "            super_feature[\"description\"]\n",
    "        )\n",
    "\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            config=types.GenerateContentConfig(system_instruction=system_prompt),\n",
    "            contents=prompt\n",
    "        )\n",
    "\n",
    "        first_level_eval.append({\n",
    "            \"super_feature\": super_feature[\"feature\"],\n",
    "            \"super_feature_description\": super_feature[\"description\"],\n",
    "            \"feature\": feature[\"sub_feature\"],\n",
    "            \"description\": feature[\"description\"],\n",
    "            \"feature_eval\": json.loads((response.text).replace('```', '').replace('json', ''))\n",
    "        })\n",
    "\n",
    "        # Minimizar problemas de limite de requisicao por minuto\n",
    "        time.sleep(5)\n",
    "\n",
    "# Salvar os resultados em um arquivo JSON\n",
    "with open(\"../data/gemini/base_prompt/first_level_eval.json\", \"w\") as f:\n",
    "    json.dump(first_level_eval, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2 Feature Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_eval = []\n",
    "\n",
    "for super_feature in second_level_subfeatures:\n",
    "    for feature in super_feature[\"sub_features\"]:\n",
    "        prompt = get_prompt(\n",
    "            feature[\"sub_feature\"], \n",
    "            feature[\"description\"],\n",
    "            super_feature[\"feature\"],\n",
    "            super_feature[\"description\"]\n",
    "        )\n",
    "\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            config=types.GenerateContentConfig(system_instruction=system_prompt),\n",
    "            contents=prompt\n",
    "        )\n",
    "\n",
    "        second_level_eval.append({\n",
    "            \"root_feture\": super_feature[\"super_feature\"],\n",
    "            \"root_feature_description\": super_feature[\"super_feature_description\"],\n",
    "            \"super_feature\": super_feature[\"feature\"],\n",
    "            \"super_feature_description\": super_feature[\"description\"],\n",
    "            \"feature\": feature[\"sub_feature\"],\n",
    "            \"description\": feature[\"description\"],\n",
    "            \"feature_eval\": json.loads((response.text).replace('```', '').replace('json', ''))\n",
    "        })\n",
    "\n",
    "        # Minimizar problemas de limite de requisicao por minuto\n",
    "        time.sleep(5)\n",
    "\n",
    "# Salvar os resultados em um arquivo JSON\n",
    "with open(\"../data/gemini/base_prompt/second_level_eval.json\", \"w\") as f:\n",
    "    json.dump(second_level_eval, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
